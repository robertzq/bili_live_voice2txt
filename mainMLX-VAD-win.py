import subprocess
import time
import sys
import numpy as np
import threading
import queue
import warnings
import torch
from faster_whisper import WhisperModel  # ðŸ‘ˆ æ›¿æ¢äº† mlx_whisper

warnings.filterwarnings("ignore")

# ================= é…ç½®åŒº =================
ROOM_ID = "24692760" 
# Windows ä¸Šæ¨¡åž‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° C:\Users\ä½ çš„ç”¨æˆ·å\.cache\huggingface...
MODEL_SIZE = "large-v3" 
# =========================================

audio_queue = queue.Queue()
IGNORE_KEYWORDS = [
    "by bwd6", "å­—å¹•by", "Amara.org", "ä¼˜ä¼˜ç‹¬æ’­å‰§åœº", "compared compared",
    "YoYo Television", "ä¸åç‚¹èµž", "è®¢é˜…æˆ‘çš„é¢‘é“", "Copyright", "The following content"
]

# === ðŸŽ§ åˆå§‹åŒ– VAD æ¨¡åž‹ (GPU åŠ é€Ÿ) ===
print("ðŸ›  æ­£åœ¨åŠ è½½ VAD æ¨¡åž‹...")
# æ£€æŸ¥æ˜¯å¦æœ‰ NVIDIA æ˜¾å¡
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸ–¥ï¸ è¿è¡Œè®¾å¤‡: {DEVICE} (RTX 3060 Ti åº”è¯¥æ˜¾ç¤º cuda)")

vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                  model='silero_vad',
                                  force_reload=False,
                                  trust_repo=True)
(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
vad_model.to(DEVICE) # æŠŠ VAD æ¨¡åž‹ä¹Ÿä¸¢åˆ°æ˜¾å¡ä¸Š
print("âœ… VAD æ¨¡åž‹åŠ è½½å®Œæ¯•")


# === ðŸš€ åˆå§‹åŒ– Whisper æ¨¡åž‹ (Faster-Whisper) ===
print(f"ðŸš€ æ­£åœ¨åŠ è½½ Faster-Whisper ({MODEL_SIZE})...")
# compute_type="float16" æ˜¯ 3060Ti çš„ç”œç‚¹ç²¾åº¦ï¼Œé€Ÿåº¦å¿«ä¸”ç²¾åº¦ä¸æŸå¤±
whisper_model = WhisperModel(MODEL_SIZE, device="cuda", compute_type="float16")
print("âœ… Whisper æ¨¡åž‹åŠ è½½å®Œæ¯•")


def stream_producer(room_id):
    """ç”Ÿäº§è€…ï¼šè´Ÿè´£æŠ“å– Bç«™ ç›´æ’­æµ"""
    print(f"ðŸ”— [ç”Ÿäº§è€…] æ­£åœ¨è¿žæŽ¥ç›´æ’­é—´: {room_id} ...")
    
    # Windows ä¸‹ subprocess è°ƒç”¨å‘½ä»¤ï¼Œæœ‰æ—¶å€™éœ€è¦ shell=True æˆ–è€…å®Œæ•´çš„ exe è·¯å¾„
    # å¦‚æžœæŠ¥é”™æ‰¾ä¸åˆ°å‘½ä»¤ï¼Œè¯·ç¡®ä¿ streamlink å’Œ ffmpeg åœ¨çŽ¯å¢ƒå˜é‡é‡Œ
    streamlink_cmd = ["streamlink", "--twitch-disable-ads", f"https://live.bilibili.com/{room_id}", "best", "--stdout"]
    ffmpeg_cmd = ["ffmpeg", "-i", "pipe:0", "-vn", "-ac", "1", "-ar", "16000", "-f", "s16le", "-loglevel", "quiet", "-"]
    
    try:
        # Windows ä¸Šå¯èƒ½éœ€è¦ shell=True æ¥å¯»æ‰¾å‘½ä»¤ï¼Œä½†é€šå¸¸ä¸å»ºè®®ã€‚å¦‚æžœè·‘ä¸é€šï¼Œå°è¯•æ”¹ä¸º True
        process_streamlink = subprocess.Popen(streamlink_cmd, stdout=subprocess.PIPE)
        process_ffmpeg = subprocess.Popen(ffmpeg_cmd, stdin=process_streamlink.stdout, stdout=subprocess.PIPE)
        print("ðŸŽ§ [ç”Ÿäº§è€…] éŸ³é¢‘æµå·²å»ºç«‹ï¼Œå¼€å§‹å­˜å…¥é˜Ÿåˆ—...")
        
        # åˆ‡ç‰‡æ—¶é—´
        chunk_seconds = 8 
        chunk_size = 16000 * 2 * chunk_seconds
        
        while True:
            in_bytes = process_ffmpeg.stdout.read(chunk_size)
            if not in_bytes: break
            
            # è½¬ä¸º float32
            audio_data = np.frombuffer(in_bytes, np.int16).flatten().astype(np.float32) / 32768.0
            audio_queue.put(audio_data)
            
    except Exception as e:
        print(f"ç”Ÿäº§è€…å‡ºé”™: {e}")
        print("âš ï¸ æç¤ºï¼šå¦‚æžœåœ¨ Windows ä¸ŠæŠ¥é”™æ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ FFmpeg æ˜¯å¦æ·»åŠ åˆ°äº†çŽ¯å¢ƒå˜é‡ Path ä¸­")
    finally:
        if 'process_ffmpeg' in locals(): process_ffmpeg.kill()
        if 'process_streamlink' in locals(): process_streamlink.kill()

def is_hallucination(text):
    for kw in IGNORE_KEYWORDS:
        if kw.lower() in text.lower():
            return True
    return False

def check_voice_activity(audio_np, model):
    # numpy -> tensor -> gpu
    audio_tensor = torch.from_numpy(audio_np).to(DEVICE)
    
    # èŽ·å–è¯­éŸ³æ—¶é—´æˆ³
    speech_timestamps = get_speech_timestamps(audio_tensor, model, sampling_rate=16000)
    
    if not speech_timestamps:
        return False
    
    total_speech_time = sum([(i['end'] - i['start']) for i in speech_timestamps]) / 16000
    return total_speech_time > 0.5

def main():
    t = threading.Thread(target=stream_producer, args=(ROOM_ID,), daemon=True)
    t.start()
    
    log_file = f"{ROOM_ID}_win_log_{int(time.time())}.txt"
    last_text = ""
    
    print("ðŸ¤– [æ¶ˆè´¹è€…] å¼•æ“Žå¯åŠ¨ (CUDA åŠ é€Ÿä¸­)...")

    while True:
        try:
            audio_data = audio_queue.get()
            
            # === ðŸ›‘ VAD æ£€æµ‹ ===
            if not check_voice_activity(audio_data, vad_model):
                print(f"ðŸŽµ [VAD] é™éŸ³/çº¯éŸ³ä¹ï¼Œè·³è¿‡...")
                continue 
            
            # === âš¡ï¸ Whisper è½¬å†™ (CUDA) ===
            start_t = time.time()
            
            # faster-whisper çš„è°ƒç”¨æ–¹å¼ç•¥æœ‰ä¸åŒ
            # beam_size=5 æ˜¯æ ‡å‡†ç²¾åº¦ï¼Œå¦‚æžœæƒ³è¦æ›´å¿«å¯ä»¥è®¾ä¸º 1
            segments, info = whisper_model.transcribe(
                audio_data, 
                beam_size=5, 
                language="zh",
                vad_filter=False, # æˆ‘ä»¬è‡ªå·±åšäº† VADï¼Œæ‰€ä»¥è¿™é‡Œå…³æŽ‰å†…ç½®çš„
                no_speech_threshold=0.4,
                log_prob_threshold=-0.8
            )
            
            # faster-whisper è¿”å›žçš„æ˜¯ç”Ÿæˆå™¨ï¼Œéœ€è¦éåŽ†å‡ºæ¥
            text = "".join([segment.text for segment in segments]).strip()
            
            if len(text) > 1 and text != last_text and not is_hallucination(text):
                cost_time = time.time() - start_t
                timestamp = time.strftime("%H:%M:%S")
                line = f"[{timestamp}] (ðŸš€{cost_time:.2f}s) {text}"
                print(line)
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write(line + "\n")
                last_text = text
                        
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"Error: {e}")

if __name__ == "__main__":
    main()