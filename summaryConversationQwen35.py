import requests
import json
import os
import sys

# --- å…³é”®ï¼šå¼ºåˆ¶å…³é—­å½“å‰è¿›ç¨‹çš„ä»£ç†è®¾ç½® ---
os.environ['no_proxy'] = 'localhost,127.0.0.1'
if "http_proxy" in os.environ: del os.environ["http_proxy"]
if "https_proxy" in os.environ: del os.environ["https_proxy"]

# --- é…ç½®åŒºï¼šæŒ‡å‘å…¨æ–°çš„ llama-server ---
MODEL = "Qwen3.5-35B-A3B" # åå­—å¯ä»¥éšä¾¿å†™ï¼Œå› ä¸ºæœåŠ¡å™¨åªåŠ è½½äº†è¿™ä¸€ä¸ªæ¨¡å‹
FILE_PATH = "æŸšé”–å­_1894720970_mlx_log_1772170784.txt" # æ›¿æ¢ä¸ºä½ çš„çœŸå®æ–‡ä»¶å
LLAMA_SERVER_URL = "http://127.0.0.1:8000/v1/chat/completions" # OpenAI å…¼å®¹ç«¯ç‚¹

try:
    with open(FILE_PATH, "r", encoding="utf-8") as f:
        content = f.read()
except FileNotFoundError:
    print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {FILE_PATH}")
    sys.exit()

prompt = f"""
ä½ ç°åœ¨æ˜¯ä¸€åèµ„æ·±çš„ B ç«™ç›´æ’­è§‚å¯Ÿå‘˜ã€‚è¯·é’ˆå¯¹ä»¥ä¸‹ã€æŸšé”–å­ã€‘çš„ç›´æ’­å½•éŸ³æ–‡æœ¬ï¼Œè¿›è¡Œä¸€æ¬¡â€œæ‰‹æœ¯çº§â€çš„æ·±åº¦æ€»ç»“ã€‚

### ä»»åŠ¡è¦æ±‚ï¼š
1. **æ—¶é—´çº¿è¿˜åŸ**ï¼šæŒ‰ç…§ç›´æ’­è¿›è¡Œçš„é¡ºåºï¼Œæ¢³ç†å‡ºè‡³å°‘ 5-8 ä¸ªå…³é”®çš„æ—¶é—´èŠ‚ç‚¹å’Œå¯¹åº”å‘ç”Ÿçš„äº‹ä»¶ã€‚
2. **æ ¸å¿ƒæ§½ç‚¹/æ¢—**ï¼šæå–ç›´æ’­é—´å‡ºç°çš„ç‰¹å®šé»‘è¯ã€æ¢—ï¼ˆæ¯”å¦‚æåˆ°çš„â€œå‡‰èœâ€ã€â€œæ²™å°˜æš´â€å…·ä½“æ˜¯æ€ä¹ˆå›äº‹ï¼‰ã€‚
3. **å…³é”®äººç‰©ç”»åƒ**ï¼šé™¤äº†ä¸»æ’­ï¼Œæåˆ°äº†å“ªäº›é‡è¦çš„ç²‰ä¸æˆ–è§‚ä¼—ï¼ˆå¦‚â€œç»å‘½å±±ä¸»â€ã€â€œå¦™ç¬”ç”ŸèŠ±â€ï¼‰ï¼Œä»–ä»¬è¯´äº†ä»€ä¹ˆé‡è¦çš„è¯ï¼Ÿ
4. **æƒ…æ„Ÿæ›²çº¿**ï¼šä¸»æ’­ä»Šå¤©çš„æƒ…ç»ªçŠ¶æ€å¦‚ä½•ï¼Ÿï¼ˆæ¯”å¦‚ï¼šç–²æƒ«ã€å…´å¥‹ã€è¿˜æ˜¯åœ¨ç”»é¥¼ï¼Ÿï¼‰
5. **ç¡¬æ ¸ç»†èŠ‚**ï¼šä¸è¦è¯´â€œæåˆ°äº†ä¸€äº›æ•æ„Ÿé—®é¢˜â€ï¼Œæ ¸å¿ƒäº‰è®®ä¸å®è´¨å›åº”ï¼ˆç¡¬æ ¸ç»†èŠ‚ï¼‰ï¼šæœç»ä½¿ç”¨â€œå›åº”äº†è¿‘æœŸäº‰è®®â€ã€â€œèŠäº†æ•æ„Ÿè¯é¢˜â€è¿™ç§åºŸè¯ã€‚å¿…é¡»ç²¾ç¡®æç‚¼ï¼šä¸»æ’­å…·ä½“å›åº”äº†ä»€ä¹ˆèŠ‚å¥ï¼ˆèµ·å› ï¼‰ï¼Ÿå¥¹ç»™å‡ºçš„æœ€ç»ˆå¤„ç†åŸºè°ƒæ˜¯ä»€ä¹ˆï¼ˆå¼ºç¡¬å›æ€¼ã€å¦¥åé“æ­‰ã€è¿˜æ˜¯ç”»çº¿ç«‹è§„çŸ©ï¼‰ï¼Ÿå¹¶æ‘˜å½• 1-2 å¥æœ€èƒ½ä»£è¡¨å¥¹æ€åº¦çš„åŸè¯æˆ–æš´è¨€ã€‚

### å¾…åˆ†ææ–‡æœ¬ï¼š
{content}

æœ€åï¼Œè¯·ç”Ÿæˆä¸€æ®µ Mermaid æ ¼å¼çš„æ€ç»´å¯¼å›¾ä»£ç ï¼Œæ¦‚æ‹¬æœ¬æ¬¡ç›´æ’­çš„ç»“æ„ã€‚
"""

# è½¬æ¢ä¸º OpenAI å…¼å®¹çš„ Payload æ ¼å¼
payload = {
    "model": MODEL,
    "messages": [
        {"role": "user", "content": prompt}
    ],
    "stream": True,
    "temperature": 0.3
}

print(f"ğŸš€ æ­£åœ¨å‘é€è¯·æ±‚åˆ°æœ¬åœ° llama-server (ç«¯å£ 8000)...")

try:
    # å¢åŠ  proxies={'http': None, 'https': None} åŒé‡ä¿é™©
    response = requests.post(LLAMA_SERVER_URL, json=payload, stream=True, proxies={'http': None, 'https': None})
    
    # å¦‚æœ HTTP çŠ¶æ€ç ä¸æ˜¯ 200ï¼Œç›´æ¥æ‰“å°å‡ºæ¥
    if response.status_code != 200:
        print(f"âŒ æœåŠ¡å™¨è¿”å›é”™è¯¯ï¼Œä»£ç : {response.status_code}")
        print(response.text)
        sys.exit()

    # è§£æ Server-Sent Events (SSE) æ•°æ®æµ
    for line in response.iter_lines():
        if line:
            line_str = line.decode('utf-8')
            # SSE åè®®çš„ç‰¹å¾ï¼šæ•°æ®ä»¥ "data: " å¼€å¤´
            if line_str.startswith("data: "):
                data_str = line_str[6:] # å‰¥ç¦»å‰ç¼€
                
                # ç»“æŸæ ‡å¿—
                if data_str == "[DONE]":
                    print("\n\nâœ… æ€»ç»“å®Œæˆï¼")
                    break
                
                try:
                    chunk = json.loads(data_str)
                    # æå–å¢é‡æ–‡æœ¬ï¼Œé˜²ç©ºæŒ‡é’ˆ
                    delta = chunk.get("choices", [{}])[0].get("delta", {})
                    if "content" in delta:
                        # ä½ ä¼šåœ¨è¿™é‡Œå®æ—¶çœ‹åˆ° <think> è¿‡ç¨‹å’Œæœ€ç»ˆè¾“å‡º
                        print(delta["content"], end="", flush=True)
                except json.JSONDecodeError:
                    print(f"\nâš ï¸ æ”¶åˆ°é JSON æ•°æ®: {data_str}")

except requests.exceptions.ConnectionError:
    print("âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡ã€‚è¯·ç¡®ä¿ä½ è¿è¡Œäº† './start_qwen.sh' å¹¶ä¸”ç«¯å£ 8000 å¯ç”¨ã€‚")